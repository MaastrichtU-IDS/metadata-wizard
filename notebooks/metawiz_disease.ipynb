{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the annotated file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ncbi = pd.read_csv('NCBIfullset.tsv', sep = '\\t', header = None)\n",
    "input_ncbi.columns = ['pubmed_id','value','class','id_ontology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ncbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_all_diseases = input_ncbi[~input_ncbi['value'].isnull()]\n",
    "table_all_diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_raw = data[data['value'].isnull()]['pubmed_id']\n",
    "vec_split = []\n",
    "for line in vec_raw:\n",
    "    vec_split.append(line.split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.DataFrame(vec_split)\n",
    "corpus.columns = ['pubmed_id','type','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles = corpus[corpus['type'] == 't'][['pubmed_id','text']]\n",
    "df_abstracts = corpus[corpus['type'] == 'a'][['pubmed_id','text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus Approach\n",
    "\n",
    "If the same disease name is annotated to more than one ontology classes - then a new synthetic class is assigned which is the one which occurs more number of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(table_all_diseases, values = 'pubmed_id', index='value', columns='class', aggfunc= 'count')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.reset_index()\n",
    "#for new synthetic class\n",
    "table[\"synthetic_class\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = pd.DataFrame({\"CompositeMention\": table2['CompositeMention'], \"DiseaseClass\":  table2['DiseaseClass'], \n",
    "                      \"Modifier\": table2['Modifier'], \"SpecificDisease\": table2['SpecificDisease']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops[\"new_class\"] = \"\"\n",
    "ops[\"max_max\"] = ops[[\"CompositeMention\", \"DiseaseClass\", \"Modifier\",\"SpecificDisease\"]].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = []\n",
    "for index, row in ops.iterrows():\n",
    "    if (row['CompositeMention']==row['max_max']):\n",
    "        lis.append('CompositeMention')\n",
    "    elif (row['DiseaseClass']==row['max_max']):\n",
    "        lis.append('DiseaseClass')\n",
    "    elif (row['Modifier']==row['max_max']):\n",
    "        lis.append('Modifer')\n",
    "    else:\n",
    "        lis.append('SpecificDisease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append the synthetic class\n",
    "table[\"synthetic_class\"] = lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "table.to_csv('file name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation using topic modeling\n",
    "#### Warning: Does not work in windows without Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "data = tc.SFrame('diseases_all_consensus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_classes = [i for i in data['synthetic_class'].unique()]\n",
    "disease_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Feature using topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and convert to bag of words\n",
    "    doc = tc.text_analytics.count_words(data['value'])\n",
    "    doc = doc.dict_trim_by_keys(tc.text_analytics.stop_words(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn topic model\n",
    "    model = tc.topic_model.create(doc, initial_topics=text_topics['word'], verbose=False)\n",
    "    # Agreaggate the unique words\n",
    "    sf_topics = model.get_topics()\n",
    "    sf_topics = sf_topics.append(text_topics)\n",
    "    sf_words = sf_topics.groupby(key_column_names='word', operations={'sum_scores': tc.aggregate.SUM('score')})\n",
    "    \n",
    "    # Sort the features scores and filter out all those which are key\n",
    "    sf_words = sf_words.sort('sum_scores', ascending= False).filter_by(disease_classes, 'word', exclude=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_words.print_rows(50,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(category_df, n_features):\n",
    "    # Remove stopwords and convert to bag of words\n",
    "    doc = tc.text_analytics.count_words(data['value'])\n",
    "    doc = doc.dict_trim_by_keys(tc.text_analytics.stop_words(), True)\n",
    "    \n",
    "    # Learn topic model\n",
    "    model = tc.topic_model.create(doc, initial_topics=text_topics['word'], verbose=False)\n",
    "    # Agreaggate the unique words\n",
    "    sf_topics = model.get_topics()\n",
    "    #append the topics from abstract text\n",
    "    \n",
    "    sf_topics = sf_topics.append(text_topics)\n",
    "    sf_words = sf_topics.groupby(key_column_names='word', operations={'sum_scores': tc.aggregate.SUM('score')})\n",
    "    \n",
    "    # Sort the features scores and filter out all those which are key\n",
    "    sf_words = sf_words.sort('sum_scores', ascending= False).filter_by(disease_classes, 'word', exclude=True)\n",
    "    \n",
    "    # Take a look of the features related with this key class\n",
    "    features = [i for i in sf_words['word']][0:n_features] #changable\n",
    "    return sf_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_words, features = create_features(data['value'], len(sf_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make input matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_matrix(features, category_df, sf_words):\n",
    "    tuples = []\n",
    "    for word in features:\n",
    "        feature_vector = [1 if (word in i) else 0 for i in data['value']]\n",
    "        tuples.append((word, feature_vector))\n",
    "        \n",
    "    sf_features = tc.SFrame({key: value for (key, value) in tuples})\n",
    "    #concatenating the features with the category matrix\n",
    "    category_df = category_df.add_row_number()\n",
    "    sf_features = sf_features.add_row_number()\n",
    "    final_table = category_df.join(sf_features, on='id', how='left')\n",
    "    for f in features:\n",
    "        score = sf_words[sf_words['word'] == str(f)]['sum_scores'].astype(float)[0]\n",
    "        final_table[str(f)] = [(1.0+score) * i for i in final_table[str(f)]]\n",
    "        \n",
    "    return final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix = get_input_matrix(features, data, sf_words)\n",
    "input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix.export_csv('feature_matrix_.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from pycm import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=300\n",
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"feature_matrix_text_topics.csv\")\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,2:80].astype(float) #features\n",
    "Y = dataset[:,1] #target  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=39, activation='relu')) #dimension is number of features\n",
    "    model.add(Dense(3, activation='sigmoid')) #no of classes\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def second_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    #dimension is number of features\n",
    "    model.add(Dense(20, input_dim=78, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation='sigmoid')) #no of classes\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator = KerasClassifier(build_fn=second_model, epochs=10, batch_size=5, verbose=0) #baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=second_model, epochs=epochs, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "#print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X,dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = estimator.predict(X, batch_size=2, verbose=1, steps=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(actual_vector=encoded_Y, predict_vector=y_predict) # Create CM From Data\n",
    "cm.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.table\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
